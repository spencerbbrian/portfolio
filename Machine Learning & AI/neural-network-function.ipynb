{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsoGNJIWco7J"
      },
      "source": [
        "# Neural Network as function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j8F0mV7UQ74"
      },
      "outputs": [],
      "source": [
        "# Author: Brian Spencer Baiden\n",
        "# Version date: January 2025\n",
        "\n",
        "\"\"\" MLP or fully connected neural network with dynamic Structure developped from scratch\n",
        "This code example is provided in the frame of the Advanced Machine Learning JUNIA-M1 course.\n",
        "It demonstrates how to build an MLP and code it from scratch. The code permits to solve logic functions but\n",
        "can be easily adapted to sovle other non linear functions. It offers the possibility to build different MLP structures by\n",
        "changing dynamically the depth of the MLP (number of hidden layers) and the size of each layer (number of neurones).\n",
        "It offers implementation of several activation functions, optimizers and kernel weights initializers.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1+math.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    if x <0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def tanh(x):\n",
        "    return math.tanh(x)\n",
        "\n",
        "sigmoid_v = np.vectorize(sigmoid)\n",
        "relu_v = np.vectorize(relu)\n",
        "tanh_v = np.vectorize(tanh)\n",
        "sqrt_v = np.vectorize(math.sqrt)\n",
        "log_v = np.vectorize(math.log)\n",
        "\n",
        "def activation_fct (x, name):\n",
        "    if name == 'sigmoid':\n",
        "        return sigmoid_v(x)\n",
        "    elif name == 'relu':\n",
        "        return relu_v(x)\n",
        "    elif name == 'tanh':\n",
        "        return tanh_v(x)\n",
        "\n",
        "def der_activation_fct (x, name):\n",
        "    if name == 'sigmoid':\n",
        "        return x*(1-x)\n",
        "    elif name == 'relu':\n",
        "        return relu_v(x) #np.full(x.shape,1)\n",
        "    elif name == 'tanh':\n",
        "        return 1-(x*x)\n",
        "\n",
        "def entropy_fct(c,o, eps=1e-8):\n",
        "    loss = 0\n",
        "    for i in range (c.shape[0]):\n",
        "        if c[i] == 0:\n",
        "            loss = loss + abs(math.log((1-o[i]) + eps)) # add epsilon to avoid undefined values\n",
        "        else:\n",
        "            loss = loss + abs(math.log(o[i] + eps))\n",
        "\n",
        "    return loss / c.shape[0]\n",
        "\n",
        "# change the order of appearance of the 4 examples\n",
        "def toshuffle(X,C, shuffle):\n",
        "    if shuffle:\n",
        "        # shuffle X and C same way\n",
        "        randomize = np.arange(len(C))\n",
        "        np.random.shuffle(randomize)\n",
        "        X = X[randomize]\n",
        "        C = C[randomize]\n",
        "\n",
        "    return X,C\n",
        "\n",
        "def model_setinput(inputdim, dense, activation, initializer):\n",
        "    model = []\n",
        "    # first hidden layer (weights and baias)\n",
        "    #W = np.random.uniform(-w_range,w_range,[inputdim,dense])\n",
        "    if initializer == 'normal':\n",
        "        W = np.random.normal(0,1,[inputdim,dense])\n",
        "        B = np.zeros([dense,1])\n",
        "    elif initializer == 'constant':\n",
        "        W = np.full((inputdim,dense), 0.05)\n",
        "        B = np.full((dense,1),0.05)\n",
        "    elif initializer == 'uniform':\n",
        "        W = np.random.uniform(-0.05,0.05,[inputdim,dense])\n",
        "        B = np.random.uniform(-0.05,0.05,[dense,1])\n",
        "    #elif initializer == 'glorot': #### to do\n",
        "    # associate weights to the dense layer\n",
        "    dense_weight =[]\n",
        "    dense_weight.append(W)\n",
        "    dense_weight.append(B)\n",
        "    dense_weight.append(activation)\n",
        "    model.append(dense_weight)\n",
        "    return model\n",
        "\n",
        "def model_addlayer(model, dense, activation, initializer):\n",
        "    dense_previous = model[-1][1].shape[0] # size of length of last hidden layer\n",
        "    #hidden layer (weights and baias)\n",
        "    #W = np.random.uniform(-w_range,w_range,[dense_previous,dense])\n",
        "    if initializer == 'normal':\n",
        "        W = np.random.normal(0,1,[dense_previous,dense])\n",
        "        B = np.zeros([dense,1])\n",
        "    elif initializer == 'constant':\n",
        "        W = np.full((dense_previous,dense),0.5)\n",
        "        B = np.full((dense,1),0.5)\n",
        "    elif initializer == 'uniform':\n",
        "        W = np.random.uniform(-0.5,0.5,[dense_previous,dense])\n",
        "        B = np.random.uniform(-0.5,0.5,[dense,1])\n",
        "    # associate weights to the dense layer\n",
        "    dense_weight =[]\n",
        "    dense_weight.append(W)\n",
        "    dense_weight.append(B)\n",
        "    dense_weight.append(activation)\n",
        "    model.append(dense_weight)\n",
        "    return model\n",
        "\n",
        "def model_sgd(model, gradient_list, activation_list, lr, x):\n",
        "    # need to update first layer of weights alone since linked to input data.\n",
        "    gradient     = gradient_list[0].T*x.T\n",
        "    model[0][0]  = model[0][0]  - lr*gradient  # weights\n",
        "    model[0][1] = model[0][1] - lr*gradient_list[0] # baias\n",
        "    for layer_index in range (1,len(model)):\n",
        "        gradient     = gradient_list[layer_index].T*activation_list[layer_index-1]\n",
        "        model[layer_index][0]  = model[layer_index][0]  - lr*gradient # weights\n",
        "        model[layer_index][1] = model[layer_index][1] - lr*gradient_list[layer_index] # baias\n",
        "    return model\n",
        "\n",
        "def model_adam(model, gradient_list, m, v, lr):\n",
        "    beta_1 = 0.9\n",
        "    beta_2 = 0.999\n",
        "    epsilon = 1/pow(10,8)\n",
        "    if not m and not v:\n",
        "        # define m and v with same shape as gradient_list and set their units to 0\n",
        "        for i in range (len(gradient_list)):\n",
        "            m.append(np.zeros(gradient_list[i].shape))\n",
        "            v.append(np.zeros(gradient_list[i].shape))\n",
        "    # calculate moments and update weights\n",
        "    for layer_index in range (len(gradient_list)):\n",
        "        m[layer_index] = beta_1*m[layer_index] + (1-beta_1)*gradient_list[layer_index]\n",
        "        v[layer_index] = beta_2*v[layer_index] + ((1-beta_2)*gradient_list[layer_index]*gradient_list[layer_index])\n",
        "        adam_gradient = (lr*m[layer_index])/(sqrt_v(v[layer_index])+epsilon)\n",
        "        model[layer_index][0] = model[layer_index][0] - adam_gradient.T # weights\n",
        "        model[layer_index][1] = model[layer_index][1] - adam_gradient # baias\n",
        "    return model, m, v\n",
        "\n",
        "def model_train(name, model, X,C, optimizer, lr, loss_fct, num_epochs, shuffle):\n",
        "    solved = False\n",
        "    #O = np.empty(C.shape[0])\n",
        "    # set initially to positive infinity, we suppose that the best error at this stage is too high\n",
        "    best_error = float('inf')\n",
        "    if optimizer == 'adam':\n",
        "        m = []\n",
        "        v = []\n",
        "    for epochs in range(num_epochs):\n",
        "        # We may shuffle the data if required\n",
        "        X,C = toshuffle(X,C,shuffle)\n",
        "        for example_index in range (len(C)):\n",
        "            x = X[example_index].reshape(1,2)\n",
        "            # setup activation and gradient lists.\n",
        "            activation_list = []\n",
        "            gradient_list = []\n",
        "            ######## propagation #############\n",
        "            W = model[0][0]\n",
        "            B = model[0][1]\n",
        "            transfer = x.dot(W).T+B # first hidden layer\n",
        "            activation =  activation_fct(transfer, model[0][2])\n",
        "            activation_list.append(activation)\n",
        "            for layer_index in range (1,len(model)): # forward pass\n",
        "                # calculate activation function\n",
        "                W = model[layer_index][0]\n",
        "                B = model[layer_index][1]\n",
        "                transfer = activation_list[-1].T.dot(W).T+B # next hidden layer\n",
        "                activation = activation_fct(transfer, model[layer_index][2])\n",
        "                activation_list.append(activation)\n",
        "            output = activation_list[-1]\n",
        "            if loss_fct == 'mse':\n",
        "                loss  = abs(C[example_index].reshape(1,1) - output)\n",
        "            elif loss_fct == 'entropy':\n",
        "                if C[example_index] == 0:\n",
        "                    loss = abs(math.log(1-output+0.00000001))\n",
        "                else:\n",
        "                    loss = abs(math.log(output+0.00000001))\n",
        "            gradient_output  = der_activation_fct(output, model[-1][2]).dot(loss)\n",
        "            gradient_list.append(gradient_output)\n",
        "            ######## backpropagation #############\n",
        "            for layer_index in range (len(model)-1, 0, -1):\n",
        "                W = model[layer_index][0]\n",
        "                gradient = der_activation_fct(activation_list[layer_index-1], model[layer_index-1][2])*(W.dot(gradient_list[-1]))\n",
        "                gradient_list.append(gradient)\n",
        "            ######## update weights #############\n",
        "            # gradient_list.reverse() to align with layers orders\n",
        "            gradient_list.reverse()\n",
        "            if optimizer == 'sgd':\n",
        "                model = model_sgd(model, gradient_list, activation_list, lr, x)\n",
        "            elif optimizer == 'adam':\n",
        "                model, m, v = model_adam(model, gradient_list, m, v, lr)\n",
        "        # prepare saving the best model\n",
        "        O = model_predict(model, X, False)\n",
        "        if loss_fct == 'mse':\n",
        "            # calculate MSE the Mean Squared Error\n",
        "            error = np.average(pow((C-O),2))\n",
        "        elif loss_fct == 'entropy':\n",
        "            error = entropy_fct(C,O)\n",
        "        if error < best_error:\n",
        "            best_error = error\n",
        "            # save model (weights and baias)\n",
        "            best_model = model\n",
        "        if error > 1:\n",
        "            break\n",
        "        print(\"epoch, \", loss_fct, \": \", epochs, error)\n",
        "        if np.average(pow((C-np.round(O)),2)) == 0:\n",
        "            print(name, \"solved!\")\n",
        "            print(epochs+1, \" epoch(s)\")\n",
        "            print (\"C,O \", C, O)\n",
        "            solved = True\n",
        "            break\n",
        "    return solved, best_model\n",
        "\n",
        "def model_predict(model, X, toround):\n",
        "    O = np.empty(X.shape[0])\n",
        "    for example_index in range (len(X)):\n",
        "        x = X[example_index].reshape(1,2)\n",
        "        # setup activation\n",
        "        activation_list = []\n",
        "        ######## propagation #############\n",
        "        W = model[0][0]\n",
        "        B = model[0][1]\n",
        "        transfer = x.dot(W).T+B # first hidden layer\n",
        "        activation =  activation_fct(transfer, model[0][2])\n",
        "        activation_list.append(activation)\n",
        "        for layer_index in range (1,len(model)):\n",
        "            # calculate activation function\n",
        "            W = model[layer_index][0]\n",
        "            B = model[layer_index][1]\n",
        "            transfer = activation_list[-1].T.dot(W).T+B # second hidden layer\n",
        "            activation = activation_fct(transfer, model[layer_index][2])\n",
        "            activation_list.append(activation)\n",
        "        O[example_index] = activation_list[-1]\n",
        "    if (toround):\n",
        "        return np.round(O)\n",
        "    else:\n",
        "        return O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFmRZrRUUeq4"
      },
      "outputs": [],
      "source": [
        "############# call MLP train function #################\n",
        "solved = False\n",
        "run = 0\n",
        "X = np.array(([0,0],[0,1],[1,0],[1,1]), dtype=int)#inputs\n",
        "C = np.array((0,1,1,0),dtype=int)#resultat d'un XOR\n",
        "lr = 0.1\n",
        "num_epochs = 50\n",
        "max_run = 25\n",
        "\n",
        "i = 0\n",
        "while (True): # max_run < 25\n",
        "    i += 1\n",
        "    print (\"====Run {}====\".format(max_run))\n",
        "    # Build a new model\n",
        "    model = model_setinput(2, 3, 'relu', initializer='normal')\n",
        "    model = model_addlayer(model, 4,'relu', initializer='normal')\n",
        "    model = model_addlayer(model, 1,'sigmoid', initializer='normal')\n",
        "    solved, best_model = model_train(\"XOR\", model, X,C,'sgd',lr, 'mse', num_epochs, shuffle=True)\n",
        "    #print(model_predict(best_model, X, False))\n",
        "    #print(model_predict(best_model, X, True) )\n",
        "    if solved:\n",
        "        print (\"solved :) ---- Congratulation\")\n",
        "        break\n",
        "\n",
        "    if i == max_run:\n",
        "        print('Maximum iterations reached, failed to converge !')\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeuYtgZxcxMl"
      },
      "source": [
        "# Neural Network as classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yc9hmt91gV3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "def relu(x):\n",
        "    return np.max((x, 0))\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, n_inputs, activation=None):\n",
        "\n",
        "        # weights and bias\n",
        "        self.W = (np.random.rand(n_inputs + 1) * 2) - 1   # [-1, +1]\n",
        "\n",
        "        # re-initialize bias\n",
        "        self.W[-1] = np.random.rand(1) - 0.5   # [-0.5, +0.5]\n",
        "\n",
        "        self.activation = activation\n",
        "        self.inputs = None\n",
        "        self.out = None\n",
        "        self.gradient = np.zeros(n_inputs + 1)   # gradients of weights and bias\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = np.concatenate((X, np.array((1,))))   # we add 1 as input for the bias\n",
        "        self.inputs = X\n",
        "        o = np.sum([w * x for w, x in zip(self.W, X)])\n",
        "        self.out = o\n",
        "\n",
        "        if self.activation is not None:\n",
        "            o = self.activation(o)\n",
        "\n",
        "        return o\n",
        "\n",
        "    def backward(self, next_layer, loss, idx):\n",
        "        if loss is not None: # this is last layer\n",
        "            for i in range(len(self.W)):\n",
        "                if self.activation is None:\n",
        "                    self.gradient[i] = loss * self.inputs[i]\n",
        "                else:\n",
        "                    self.gradient[i] = loss * self.inputs[i] if self.out > 0 else 0\n",
        "\n",
        "                self.W[i] = self.W[i] - learning_rate * self.gradient[i]\n",
        "        else:\n",
        "            for i in range(len(self.W)):\n",
        "                grads = 0\n",
        "                for neuron in next_layer.neurons:\n",
        "                    if self.activation is None:\n",
        "                        grads += neuron.W[idx] * neuron.W[idx].gradient * self.inputs[i]\n",
        "                    else:\n",
        "                        grads += self.out * neuron.W[idx] * neuron.gradient[idx] * self.inputs[i] if self.out > 0 else 0\n",
        "\n",
        "                # print(grads)\n",
        "                self.gradient[i] = grads\n",
        "                self.W[i] = self.W[i] - learning_rate * self.gradient[i]\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, n_inputs, n_neurons, activation=None):\n",
        "        self.neurons = [Neuron(n_inputs, activation) for i in range(n_neurons)]\n",
        "\n",
        "    def forward(self, X):\n",
        "        o = [neuron.forward(X) for neuron in self.neurons]\n",
        "        return o\n",
        "\n",
        "    def backward(self, next_layer, loss=None):\n",
        "        for idx, neuron in enumerate(self.neurons):\n",
        "            neuron.backward(next_layer, loss, idx)\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, n_inputs, layers, activations=None):\n",
        "        # layers is list of numbers of neurons in each layer\n",
        "        # activations is list of activation functions (same length as layers)\n",
        "\n",
        "        self.layers = []\n",
        "        inputs_ = n_inputs\n",
        "        for i in range(len(layers)):\n",
        "            if activations is not None:\n",
        "                self.layers.append(Layer(inputs_, layers[i], activations[i]))\n",
        "            else:\n",
        "                self.layers.append(Layer(inputs_, layers[i]))\n",
        "\n",
        "            inputs_ = layers[i]\n",
        "\n",
        "    def forward(self, X):\n",
        "        o = X\n",
        "        for layer in self.layers:\n",
        "            o = layer.forward(o)\n",
        "\n",
        "        return o\n",
        "\n",
        "    def summary(self):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            print('layer {} - {} neurons'.format(i+1, len(layer.neurons)))\n",
        "\n",
        "    def backward(self, loss):\n",
        "\n",
        "        # last layer backprop\n",
        "        self.layers[-1].backward(next_layer=None, loss=loss)\n",
        "\n",
        "        next_layer = self.layers[-1]\n",
        "        for layer in self.layers[:-1][::-1]:\n",
        "            layer.backward(next_layer=next_layer)\n",
        "            next_layer = layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2gyxvjAtgjG"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    (0, 0, 0),\n",
        "    (0, 1, 1),\n",
        "    (1, 0, 1),\n",
        "    (1, 1, 0),\n",
        "]\n",
        "\n",
        "net = Network(n_inputs=2, layers=[3, 4, 1], activations=[relu, relu, relu])\n",
        "net.summary()\n",
        "\n",
        "epochs = 1000\n",
        "for e in range(epochs):\n",
        "    for a, b, c in dataset:\n",
        "        o = net.forward((a, b))\n",
        "        loss = (c - o[0])\n",
        "        net.backward(loss)\n",
        "\n",
        "for a, b, c in dataset:\n",
        "    o = net.forward((a, b))\n",
        "    print(a, b, c, round(o[0]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
